[[Discussion]]
== Discussion
Section 9 opens up the discussion by further elaborating on findings or on raising open issues from the work.

=== Application and Process Terminology
There is a need to (re-)establish a common vocabulary for the software artefacts involved. For instance, the terms _Application_, _Application Package_, _Process_, _Service_ and _Endpoint_ lend themselves to be used interchangeably. The potential confusion is even more important in context of _Workflows_ that can chain applications and execute deployed processes.

=== Transition from WPS 1.0 to WPS 2.0
Current adoption of WPS 2.0 standard is still very limited. For instance, 52°North implementations such as https://github.com/52North/javaPS/releases/tag/v1.2.0[javaPS] are based on OGC Web Processing Service standard version 2.0; https://pywps.readthedocs.io/en/master/[PyWPS] server is currently only planning support for this version. In the near future, one can still expect to see new WPS 1.0 endpoints appear. By enabling support of pre-deployed WPS 1.0 services into a WPS 2.0 workflow environment, and onwards to WPS 3.0, the <<Solution, Solution>> presented in this report has the potential to facilitate a transition phase for legacy deployments. This relative ease to provide WPS 2.0 RESTful interfaces comes at the expense of the increased software complexity involved with an EMS/ADES implementation such as Weaver.

It has been shown with the previous workflows that interoperability between WPS 1.0 and ESGF CWT interface can be achieved with the Weaver component. If climate processes could have been available as a Docker application during the short project time frame, a workflow composed of all WPS 1.0, ESGF CWT and WPS-T 2.0 interface could have been tested also.

=== From Docker Image to Application
In the process of adapting the EMS to climate applications, it became clear that having a Docker image of an application is not a sufficient condition to consider it an _Application Package_. Docker images are often packaged with ease of installation and deployment in mind, but not runtime considerations such as execution. In PAVICS cite:[AGU2017FMIN21D0060G] and Birdhouse cite:[EGU2018EHBERECT], climate processes are packaged as Docker images, alongside a WPS server. These processes are intended to be run as a service, so that requests are made to it and result are fetched from it. The Docker container is deployed and then is always running. To convert this service into an _Application Package_ as described in cite:[TB14AP2018], the processes would need to be packaged without any web server, database or file server. The Docker image must define a _run_ command that map inputs directly to the process function. The result must then be extracted from the container after it has been run. It is a completely different paradigm that require careful consideration to define clear entry points to the executable code.

=== CWL File Formats
In CWL, tools and workflows can take _File_ types as input and produce them as output. This specification  documents the use the tool and allows simple type-checking when creating parameter files. It is possible to reference existing ontologies, like EDAM, or reference a local ontology. Additional investigation for creation of such an ontology for EO and climate is required, and recommended as future work in the <<Summary, summary>>.

=== Metalinks
One recommendation from OGC Testbed-14: ADES & EMS Results and Best Practices Engineering Report cite:[TB14ADESEMS2018] is to use Metalinks for multiple outputs. During the ESGF Compute Challenge, Ouranos and DKRZ produced an implementation that adds _MetaFile_ and _MetaLink_ classes, code that has been https://github.com/geopython/pywps/pull/466[merged in PyWPS]. The intent is to improve user experience with respect to WPS outputs storing multiple files under the same identifier. For example, when computing an indicator on an ensemble of climate simulations, the output is going to be a list of files. At the moment, a usual way to send that back to the user is to either zip these files and provide the reference to the zip file, or create a txt file storing the list of references.

In this implementation, a process developer declares an output as a _FORMATS.METALINK_, instantiates a _MetaLink_ object and fills it with all the output files and their metadata (name, description). The server returns an XML file matching the metalink schema. Metalink files are recognized and individual files downloaded and converted into objects. The _MetaLink_ format opens up interesting possibilities, including distributing files as torrents or use of a checksum. Additionally, impacts of _Metalink_ support in workflow environments is to be investigated.

=== NASA NCCS STRATUS
NASA Stratus Framework provides a set of standards and APIs to facilitate the integration of disparate analytic services into unified workflows with a common interface. The Stratus framework has been partially implemented at NCCS and a limited deployment is in the initial stages of testing and security review. As the framework addresses several challenges of EO platforms, it is presented to the reader of this report as a solution potentially complementing ESA Thematic Exploitation Platform (TEP) open architecture. Below are STRATUS primary requirements, as listed in its https://github.com/nasa-nccs-cds/stratus/blob/master/docs/STRATUS-WhitePaper-1.0.pdf[white paper]:

* Modular Endpoints: Requires a common endpoint API that can be used to wrap any analytic operator and expose it as an instantiation of a singular analytic module interface.
* Modular Layers: Requires a technology-agnostic architectural layer specification adhering to a common API, which can be instantiated using a wide range of applicable technologies.
* A common language: Requires a common request-response language for describing workflows that can be assimilated and understood by all of the architectural layers and endpoints.

=== Applicability to Machine Learning

In Testbed-15 Machine Learning tasks, there is a specification to “continue the work of cloud computing from Testbed 13, 14 with work on WPS and possibly CWL (Common Workflow Language), and continue the LiDAR best practices work from Testbed 14”. This applies for these deliverables:

* D100	Petawawa cloud mosaicking Machine Learning model
* D101	Petawawa land cover classification Machine Learning model
* D102	New Brunswick forest Machine Learning model

It is to be noted that work documented in OGC Testbed-14: Machine Learning Engineering Report cite:[TB14ML2018] presents a Dockerized, self-contained application package of PyTorch, including helper libraries. The work presented in the current report is therefore seen as potentially complementary to the ML common architecture to be defined in the Machine Learning thread.

=== Generation of CWL Wrappers for ESGF CWT API
In this work, a CWL descriptor such as the one in <<CWL_WPS1_Finch, Annex B>> is generated automatically using _DescribeProcess_ on a pre-deployed WPS 1.0 service. The process description is expected to explicitly define all inputs and outputs. In the case of ESGF CWT API compliant process, _DescribeProcess_ effectively lists _Domain_, _Variable_ and _Operation_ inputs, as specified in the API and described in <<ESGFCompute, Section 6>>. The multiple values of these parameters have to be passed separately in a JSON file. As such, they values are not automatically available for the CWL generation. A CWL file has to be manually edited to explicitly add and map the all inputs. An example can be seen in <<CWL_WPS1_EDAS, Annex C>>. To remediate this, a specific set of inputs, or profile, could be defined as a shared interface between deployed ESGF processes. Additionally, the EMS could implement special logic to parse the multiple values of an ESGF CWT API compliant service.

=== ESGF Supporting Material

The following publications offer a thorough view on various challenges and requirements of ESGF:

* Strategie Roadmap for the Earth System Grid Federation cite:[Williams2015]
* Big Data Challenges in Climate Science: Improving the next-generation cyberinfrastructure cite:[SCHNASE2016]
* Enabling Reanalysis Research Using the Collaborative Reanalysis Technical Environment (CREATE) cite:[POTTER2018]
* Requirements for a global data infrastructure in support of CMIP6 cite:[BALAJI2018]
* Addressing the massive CMIP6 data science challenge through the ESGF global federation cite:[AGU2018FMIN21A]

[[ESGFCompute]]
== ESGF Compute Challenge
=== Background information
Addressing “big data” challenges in Earth system research, ESGF is an international collaboration of computer scientists, data scientists, and climate researchers. The federation houses an enormous database of global observational and simulation data — more than 5 petabytes — and manages the high-performance computing (HPC) hardware and software infrastructure necessary for scientific climate research. In the nearly two decades since its launch, ESGF has grown to serve 25,000 users on 6 continents cite:[auten20187th].

The following publications offer a thorough view on various challenges and requirements of ESGF:

* Strategie Roadmap for the Earth System Grid Federation cite:[Williams2015]
* Big Data Challenges in Climate Science: Improving the next-generation cyberinfrastructure cite:[SCHNASE2016]
* Enabling Reanalysis Research Using the Collaborative Reanalysis Technical Environment (CREATE) cite:[POTTER2018]
* Requirements for a global data infrastructure in support of CMIP6 cite:[BALAJI2018]
* Addressing the massive CMIP6 data science challenge through the ESGF global federation cite:[AGU2018FMIN21A]

As of early 2019, the priorities of ESGF are as follows:

* Containerized architecture
* OAuth 2.0 deployments
* CMIP6 data replication
* Next-generation search services
* Scalability on multiple fronts
* Machine Learning tools
* Compute nodes challenges

On that last element, the Compute Working Team (CWT) currently conducts iterative deployments and tests to stand up multiple production-ready, officially certified ESGF compute nodes. This effort includes data selection, security scans, API development, scalability tests, documentation updates, and regular status reports. The current Engineering Report is one of the contributions to this challenge.

=== Implementations

The ESGF Compute Challenge participants completed various experiments and integrations. The EMS implementation described in this report is based on components from both Birdhouse and PAVICS. The <<solution, Solution>> described in this report focus on providing access to deployed ESGF CWT API processes and providers in application packages and workflows. Future work includes exposing application packages through ESGF CWT API, either manually or dynamically. Below is a list of the various systems and frameworks involved in the Compute Challenge.

* https://computation.llnl.gov/projects/aims-analytics-and-informatics-management-systems[AIMS] - Analytics and Informatics Management Systems, LLNL.
* http://bird-house.github.io/[Birdhouse] Framework, DKRZ/CEDA/IPSL. See cite:[EGU2018EHBERECT].
* https://climate4impact.eu/impactportal/general/index.jsp[C4I] - Climate4Impact, CERFACS/IPSL/KNMI/CMCC.
* https://www.nccs.nasa.gov/services/analytics/EDAS[EDAS] - Earth Data Analytic Services Framework, NASA Goddard SFC. See cite:[AGU2018MAXWELL].
* https://github.com/sci-visus/OpenVisus[OpenVisus], University of Utah. See cite:[PetruzzaVGSFAPB17].
* https://github.com/OphidiaBigData/ophidia-analytics-framework[Ophidia] Analytics Framework, CMCC. See cite:[FIORE2016].
* https://ouranosinc.github.io/pavics-sdi/[PAVICS] - Platform for the Analysis and Visualization of Climate Science, Ouranos/CRIM. See cite:[AGU2017FMIN21D0060G].

As of early April, only AIMS and EDAS offered endpoint implementing the ESGF Compute Working Team API. Interoperability experiments conducted on these providers for this work are described in <<Solution, Section 7>> and <<TIEs, Section 8>>.

=== ESGF Compute Working Team API

A reference server implementation of a https://github.com/ESGF/esgf-compute-wps[Compute Node] is also offered, while certifications datasets are listed in a https://docs.google.com/document/d/1pxz1Kd3JHfFp8vR2JCVBfApbsHmbUQQstifhGNdc6U0/edit?usp=sharing[GoogleDocs] living document. The process requirements are defined https://github.com/ESGF/esgf-compute-api/blob/devel/docs/source/cwt.compat.rst[on GitHub]. These are partially duplicated below for convenience.

=== Inputs
The datainputs parameter will consist of the following three types.

* Domain
* Variable
* Operation

==== Domain
This WPS input should use the identifier domain. The input will be passed an array of domains that are comprised of one or more dimensions.

* id [Required]
* mask [Optional]
* One ore more dimensions keyed using a descriptive identifier. [Required]

==== Variable
This WPS input should use the identifier variable. The input will be passed an array of variables that define all inputs for the process.

* id [Required] - Can be extended with a | followed by an identifier that will be used to reference the Variable
* uri [Required]
* domain [Optional]

==== Operation
This WPS input should use the identifier operation. The input will be passed an array of operations.

* name [Required]
* input [Required] - List of inputs
* result [Optional] - Name that can be referenced by other operations when creating workflows
* domain [Optional]
* axes [Optional]
* gridder [Optional]
* Zero or more additional parameters [Optional]

=== Output
The WPS process should only have a single output whose identifier is output.

* uri [Required]
* id [Optional]
* domain [Optional]
* mime-type [Optional]
